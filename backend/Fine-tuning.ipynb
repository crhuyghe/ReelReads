{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMBO8Ja8f9LRsB9MqjFKhX7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uiSFnKbZ-tBs"},"outputs":[],"source":["!pip install datasets\n","!pip install torch\n","!pip install -q -U transformers accelerate\n","!pip install transformers[torch]\n","!pip install pandas\n","!pip install langdetect"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n","from datasets import Dataset\n","import pandas as pd\n","import torch\n","from langdetect import detect, LangDetectException\n","from google.colab import drive\n","\n","# Function to detect language and filter non-English rows\n","def filter_english(df, text_column):\n","    def is_english(text):\n","        try:\n","            return detect(text) == 'en'\n","        except LangDetectException:\n","            return False\n","    return df[df[text_column].apply(is_english)]\n","\n","drive.mount('/content/drive/')\n","\n","# Load first movie dataset\n","datasetMovieOne = pd.read_csv('/content/drive/My Drive/Fine-tuning/movies_metadata.csv', low_memory=False)\n","datasetMovieOne = datasetMovieOne.dropna(subset=['title', 'overview', 'vote_average', 'vote_count']).reset_index(drop=True)\n","datasetMovieOne = filter_english(datasetMovieOne, 'overview')\n","datasetMovieOne = datasetMovieOne[['title', 'overview', 'vote_average', 'vote_count']]\n","\n","# Load second movie dataset\n","df1 = pd.read_csv('/content/drive/My Drive/Fine-tuning/tmdb_5000_credits.csv')\n","df2 = pd.read_csv('/content/drive/My Drive/Fine-tuning/tmdb_5000_movies.csv')\n","df1.columns = ['id', 'title', 'cast', 'crew']\n","datasetMovieTwo = df2.merge(df1, on='id')\n","datasetMovieTwo = datasetMovieTwo.dropna(subset=['original_title', 'overview', 'vote_average', 'vote_count']).reset_index(drop=True)\n","datasetMovieTwo = filter_english(datasetMovieTwo, 'overview')\n","datasetMovieTwo = datasetMovieTwo[['original_title', 'overview', 'vote_average', 'vote_count']]\n","\n","# Load the book dataset\n","df1 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data1.csv')\n","df2 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data2.csv')\n","df3 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data3.csv')\n","df4 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data4.csv')\n","df5 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data5.csv')\n","df6 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data6.csv')\n","df7 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data7.csv')\n","df8 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data8.csv')\n","df9 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data9.csv')\n","df10 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data10.csv')\n","df11 = pd.read_csv('/content/drive/My Drive/Fine-tuning/book_data11.csv')\n","datasetBooks = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11], ignore_index=True)\n","datasetBooks = datasetBooks.dropna(subset=['Name', 'Description', 'Rating', 'CountsOfReview']).reset_index(drop=True)\n","datasetBooks = datasetBooks.sample(n=50000, random_state=42).reset_index(drop=True)\n","datasetBooks = filter_english(datasetBooks, 'Description')\n","datasetBooks = datasetBooks[['Name', 'Description', 'Rating', 'CountsOfReview']]\n","print(\"grabbed datasets\")"],"metadata":{"id":"rQG6V5Vh-1Bt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract titles, overviews, and ratings for tokenizing\n","def extract_text(df, text_columns):\n","    return df[text_columns].apply(lambda x: ' '.join(x.dropna()), axis=1).tolist()\n","\n","movie_one_texts = extract_text(datasetMovieOne, ['title', 'overview'])\n","movie_two_texts = extract_text(datasetMovieTwo, ['original_title', 'overview'])\n","book_texts = extract_text(datasetBooks, ['Name', 'Description'])\n","all_texts = movie_one_texts + movie_two_texts + book_texts\n","print(\"finished extracting and combining text\")\n","\n","# Tokenize the text data\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n","def tokenize_function(examples):\n","    return tokenizer(examples, truncation=True, padding=\"max_length\", max_length=512)\n","tokenized_texts = [tokenize_function(text) for text in all_texts]\n","input_ids = torch.tensor([t[\"input_ids\"] for t in tokenized_texts])\n","attention_mask = torch.tensor([t[\"attention_mask\"] for t in tokenized_texts])\n","tokenized_dataset = Dataset.from_dict({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n","print(\"finished tokenizing\")"],"metadata":{"id":"hBDNlZ7n_K7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n","training_args = TrainingArguments(\n","    num_train_epochs=3,\n","    per_device_train_batch_size=32,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    output_dir='/content/drive/My Drive/Fine-tuning/model_results',\n",")\n","model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset,\n",")\n","trainer.train()\n","print(\"finished training\")\n","\n","# Save the model\n","trainer.save_model(\"bert_fine-tuned\")"],"metadata":{"id":"seOQC7Pg_Nez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install huggingface_hub\n","from google.colab import userdata\n","\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","model.push_to_hub(\"emma7897/CSI4999\", token = HF_TOKEN)\n","tokenizer.push_to_hub(\"emma7897/CSI4999\", token = HF_TOKEN)"],"metadata":{"id":"xcLGiVz1UI11"},"execution_count":null,"outputs":[]}]}